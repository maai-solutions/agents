# MCP Vector Store Server Configuration

# ============================================================================
# Weaviate Configuration
# ============================================================================
WV_HTTP_HOST=localhost
WV_HTTP_PORT=18080
WV_HTTP_SCHEME=http

WV_GRPC_HOST=localhost
WV_GRPC_PORT=50051
WV_GRPC_SCHEME=http

# Default collection to search
WV_COLLECTION=arag_dev

# Search parameters
WV_MAX_DISTANCE=0.7  # Maximum vector distance (0.0-1.0, lower = more similar)
WV_ALPHA=0.5         # Hybrid search balance (0.0 = keyword only, 1.0 = vector only)
WV_LIMIT=5           # Maximum number of results to return

# Embedding model (must be available in your LLM provider)
WV_EMBEDDING_MODEL=nomic-embed-text:latest

# ============================================================================
# LLM Configuration (for generating embeddings)
# ============================================================================

# For Ollama (local)
LLM_API_BASE=http://localhost:11434/v1
LLM_MODEL=gemma3:27b
LLM_API_KEY=not-needed

# For OpenAI
# LLM_API_BASE=https://api.openai.com/v1
# LLM_MODEL=gpt-4
# LLM_API_KEY=sk-your-api-key-here

# For other OpenAI-compatible providers
# LLM_API_BASE=https://your-provider.com/v1
# LLM_MODEL=your-model-name
# LLM_API_KEY=your-api-key

# ============================================================================
# Telemetry (Optional)
# ============================================================================
TELEMETRY_ENABLED=false

# If using Langfuse
# TELEMETRY_EXPORTER=langfuse
# LANGFUSE_PUBLIC_KEY=pk-lf-...
# LANGFUSE_SECRET_KEY=sk-lf-...
# LANGFUSE_HOST=https://cloud.langfuse.com

# If using OpenTelemetry
# TELEMETRY_EXPORTER=otlp
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# If using Jaeger
# TELEMETRY_EXPORTER=jaeger
# JAEGER_AGENT_HOST=localhost
